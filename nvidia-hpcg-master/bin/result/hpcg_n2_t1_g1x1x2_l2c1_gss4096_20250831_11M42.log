################################################################
### Test executed at: Sun Aug 31 11:39:43 AM CST 2025
### Command:
srun -n 2 --mpi=pmi2 --cpus-per-task=1 --export=ALL,I_MPI_HYDRA_BOOTSTRAP=slurm,I_MPI_PMI_PROVIDER=pmi2,I_MPI_PMI_LIBRARY=/slurm/libpmi2.so.0.0.0 ./hpcg.sh                         --nx 256 --ny 256 --nz 1024 --rt 300                         --p2p 4 --gpu-affinity 0:1 --b 1                         --npx 1 --npy 1 --npz 2                         --l2cmp 1                         --gss 4096
################################################################

MPI startup(): I_MPI_PMI_PROVIDER environment variable is not supported.
MPI startup(): Similar variables:
	 I_MPI_OFI_PROVIDER
	 I_MPI_CXX_PROFILE
	 I_MPI_FC_PROFILE
	 I_MPI_F77_PROFILE
	 I_MPI_F90_PROFILE
MPI startup(): To check the list of supported variables, use the impi_info utility or refer to https://www.intel.com/content/www/us/en/docs/mpi-library/developer-reference-linux/
HPCG-NVIDIA 24.09.0  -- NVIDIA accelerated HPCG benchmark -- NVIDIA
Build v0.5.9 

Start of application (GPU-Only) ...
 | Benchmark Mode !!!! CPU reference code is not performed 
 | L2 compression is activated !!!! Currently, it is not legal to submit HPCG results with L2 compression
CUDART: cudaMalloc((void**) &(A->sellAPermColumns), sizeof(local_int_t) * (paddedRowLen * HPCG_MAX_ROW_LEN + slice_size * HPCG_MAX_ROW_LEN)) = 2 (out of memory) on v00 at (src/CudaKernels.cu:269)
srun: error: v00: task 0: Exited with exit code 1
CUDART: cudaMalloc((void**) &(A->sellAPermColumns), sizeof(local_int_t) * (paddedRowLen * HPCG_MAX_ROW_LEN + slice_size * HPCG_MAX_ROW_LEN)) = 2 (out of memory) on v00 at (src/CudaKernels.cu:269)
srun: error: v00: task 1: Exited with exit code 1
