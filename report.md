## nvidia-hpcg测试


### 下载

由于集群自带的nchpc中我没有找到nvidiahpcg，我最终再本地下载了github的源代码并直接拷贝到集群

### 安装

- 集群上的GPU为V100，也就是更早的sm_70的架构，但是默认的make文件主要针对的是架构为ampere和tesla架构的显卡。所以这是我们需要注意的很重要的一点
- 集群上的cpu型号为Intel(R) Xeon(R) Platinum 8358 CPU @ 2.60GHz，虽然我们的计算速度主要依赖的是GPU，但是CPU也起到了MPI的调度作用

### 编译

按照官方文档，我们测试的时候需要编译几个主要的环境变量。MPI, CUDA，toolkit, CUDA Mathlibs, NCCL, and NVPL Sparse，以及对应的启动标签
USE_CUDA,USE_GRACE,USE_NCCL  
这由于这一版我们需要GPU来运行，所以数学库我们必须cuda自带的数学库，相应的，我们的编译器也只能使用  
里spack包管理，我们很轻易地可以查找到相应的包安装的路径  
这里由于nvhpc自带的MPI总是出现编译错误，显示无法找到mpi.h头文件，我在检查的时候发现nvhpc中的mpi似乎没有对应的include文件夹和lib文件夹，于是我最终选择采用openmpi和intel-oneapi-mpi进行通信  
但是接下来修改完环境变量之后，我使用自带的build_sample.sh无法正确识别我的环境变量，在编译过程中总是会执行需要nvpl包的make版本，所以我选择修改完之后进行手动编译，并最终利用nvcc进行编译所有文件同时进行链接  
编译完成后我们完成了得到了一个xhpcg的可执行文件，并且这一版本我们使用cuda和nccl  

### baseline的测试

我们选择开源的hpcg-benchmark进行优化，编译和之前比较类似也就不加以赘述了，我们得到了一个纯粹的cpu运行的版本默认生成的问题规模为104*104*104，最终的计算结果为1.68GFLOPS

### 运行及优化
#### 运行方式

我整理了一个env.sh在项目根目录下，用于环境配置,主要通过spack包来设置具体的安装路径以及对应的包  
在bin目录下的run.sh配置了优化的脚本，主要涉及MPI进程数，GPU数量，运行时间问题规模，gss参数，是否启用l2cmp等进行配置，一次性可以对比多个测试结果
在根目录下的run.sh为sbatch脚本，负责具体处理任务

#### 自带脚本 
在bin目录中有一个hpcg.sh脚本来提供大量的选项来支持更多的脚本配置，相对而言提供了更加丰富的编译选项，可以为什么提供更加强大的cpu亲和性和gpu亲和性。我们来启动的最初的编译选项  
>srun -n 2 --mpi=pmi2 --cpus-per-task=1 --export=ALL ./hpcg.sh/
>    --nx 256 --ny 256 --nz 256 --rt 120/
>    --p2p 4 --gpu-affinity 0:1 --b 1/
这个启动命令说明我们输入的问题规模为256*256*256，并且同时启用了两块GPU，GPU之间启用了p2p通信协议，从而能够使用nccl进行通信
这一版的运行结果中我们的速度达到了209.161GFLOPs,和V100节点上纯粹的CPU版本相比，速度明显得到了提高，
由于这个问题主要集中在内存带宽上，所以最主要的部分集中在提高问题的规模上，经过反复测试，单块GPU最大的容量为nz=544，尽管此时计算出来的及俄国此时的存储的显存达到了29.7G左右，几乎利用了整个显存。
>--npx 1 --npy 1 --npz 2
随后我们通过戳Z方向的切分，让两块GPU中均输入类似的数据，在global中达到的效果大致为256*256*1088的问题规模。于是我们的此时的速度差不多达到了269.92GFLOPS。
与此同时我也尝试过分块的时候让x，y方向上分别翻倍，但是最终结果证明，在Z维度上的分块是比较优秀的。
于此同时我们加入其他优化选项
>--l2cmp 1
>--gss 4096
这两个选项明确指定了启动L2缓存的压缩以及在现存内部分块，这样对于内存而言按理来说还是比较友好的，但是在实际的测试过程中，反而出现了速度下降的情况。同时gss设置为8192程序出现了显存崩溃的情况。
因此最终我们测试的结果为不启动l2cmp，启动gss。

#### MPI

由于hpcg对通信比较敏感，出去nccl之外我们还有mpi也需要一部分通信，由于一般来说intelmpi优于openmpi
接下来我们换用intel-oneapi-mpi进行mpi通信。
与此同时我们也需要测试mpi的进程数，我们分别测试了1，2，4个进程，来确定是否会对结果产生影响。
测试的结果是我们的intelmpi速度略由于openmpi版本，同时两个mpi进程结果最好，最终的速度达到了275.263GFLOPS。

#### 编译选项

除此之外，编译选项本身也是比较重要的一部分，接下来我们全部启动启动。
>nvcc -O3 \
>     -gencode arch=compute_70,code=sm_70 \
>     --use_fast_math \
>     -Xptxas "-v" \
其中最重要是针对我们的GPU架构生成对应代码的-gencode arch=compute_70,code=sm_70。不过加入后续两个编译选项对结果影响不大。

#### 编译器

针对.cu文件的编译我们有llvm和nvcc两个编译器进行测试，不过由于我们目前对llvm工具链的需求不大，也可能因为自家机器的原因，nvcc毫无疑问的速度快于llvm。

### 刷分

除此以外，我们就以最优秀的编译结果来进行刷分，连续运行1800s来进行刷分。最终的结果为280.829FLOPS